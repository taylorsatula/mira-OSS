# MIRA: Context Window Curation as Architecture

MIRA manages its context window by selecting content based on information density rather than fixed quantities. When loading segment summaries during session resumption, the `SegmentCacheLoader` accumulates segments until their combined complexity scores exceed a threshold (`segment_cache_loader.py:110-187`). Each segment receives a complexity score (1-3) during summarization, representing cognitive load. A single complex segment discussing architecture details might occupy the same budget as three simple segments about routine tasks. This means rich conversations get more representation in the context window, while trivial exchanges compress away.

Discrete memories extracted from conversations operate as independent database entities with graph relationships, embeddings, and importance scores that decay through momentum loss (`scoring_formula.sql:43-71`). Each activity day without access reduces effective access count by 5%: `access_count * 0.95^(activity_days_since_last_access)`. A memory accessed 10 times that sits idle for 20 activity days decays to 3.6 effective accesses, which gets normalized by age and compared logarithmically against a baseline of 0.02 (one access per 50 activity days). Activity days measure user engagement, not calendar time—a two-week vacation doesn't age memories. Hub score from inbound links (0.04 per link up to 10, then diminishing returns) and temporal multipliers for upcoming events (2.0x within 24 hours) add resilience, but memories scoring below 0.001 move to cold storage. Hybrid search combines BM25 and vector similarity weighted by intent (`hybrid_search.py:32-93`), and `ProactiveService` traverses typed relationship links (conflicts, supersedes, causes) to surface connected memories as hierarchical clusters (`proactive.py:156-206`).

The event system coordinates these curation mechanisms without direct coupling (`cns/core/events.py:1-300`, `cns/integration/event_bus.py:20-139`). When the orchestrator needs a system prompt, it publishes `ComposeSystemPromptEvent` with the base prompt. `WorkingMemory` responds by broadcasting `UpdateTrinketEvent` to each registered trinket, triggering parallel content generation. Trinkets publish `TrinketContentEvent` with their sections (memories, tool hints, domain knowledge), and the `SystemPromptComposer` collects everything into cached and non-cached blocks before publishing `SystemPromptComposedEvent`. The event bus is synchronous—events fire and complete immediately—but the decoupling means adding new context sources requires no changes to existing components. `TurnCompletedEvent` carries the turn number and continuum state, enabling `ToolLoaderTrinket` to auto-unload idle tools without querying the orchestrator. `SegmentTimeoutEvent` triggers collapse, which publishes `SegmentCollapsedEvent` (clearing search results from `GetContextTrinket`), followed by `ManifestUpdatedEvent` (invalidating Valkey caches). Events describe state changes and carry the changed state, preventing race conditions where handlers fetch stale data.

Tools themselves participate in context curation as both consumers and contributors. The `invokeother_tool` operates as a meta-tool that manages the tool definitions appearing in context, loading tools on-demand when their hints suggest relevance and auto-unloading after five idle turns (`tools/implementations/invokeother_tool.py:206-234`, `working_memory/trinkets/tool_loader_trinket.py:1-200`). Tool hints (name + description) remain visible in working memory while full definitions (parameters, examples, implementation details) load only when needed, achieving 80-90% reduction in tool-related tokens. Other tools inject ephemeral content that follows lifecycle rules: `getcontext_tool` publishes asynchronous search results to `GetContextTrinket`, which displays successful results until segment collapse while error messages auto-cleanup after exactly five turns based on `TurnCompletedEvent` count (`working_memory/trinkets/getcontext_trinket.py:190-216`). When `SegmentCollapsedEvent` fires, the trinket clears all search state to prevent old results from leaking into new conversation contexts. This creates a model where tool outputs don't accumulate indefinitely—they appear when relevant and vanish when their utility window closes.

Toggleable domain knowledge blocks recognize that you don't need contextual knowledge at all times. What is relevant during the workday is likely not relevant when brainstorming recipes for friends. MIRA allows users to enable domain knowledge blocks that share conversation chunks in 10-turn intervals with a Letta sleeptime agent that expertly extracts iteratively refined, less-volatile blocks of knowledge. Discrete memories have advantages during passive injection, but Letta has built a reliable and robust sleeptime agent. It is the right architectural choice to delegate functionality when someone else has done it better within their own domain.

These mechanisms work together to turn the context window into something more deliberate than a simple buffer. Segments arrive pre-scored for complexity. Memories surface with their relational graphs intact but depth-limited. Tools materialize when needed and vanish when forgotten. Domain knowledge requires explicit opt-in. Every piece of content enters through a specific selection criterion: complexity threshold, recency, relationship traversal, active usage, or user preference. The result is a system where context curation happens algorithmically at multiple layers, each optimizing for relevance and density within its domain.

---

Building MIRA has been an exercise in learning that context windows aren't just constraints to work around. They're design surfaces that shape how systems think and remember. Every pattern here emerged from confronting real problems: conversations that lost coherence, memories that cluttered uselessly, tools that consumed tokens without earning their keep. If these patterns prove useful to others building conversational systems, or if they inspire better approaches we haven't considered, that would make the work worthwhile.

This codebase is a best-effort approximation of the human theater of mind. There is work still to be done, but it is already very interesting and,, unique,, to interact with. This has been a relentless labor to build. MIRA started out as a decision-tree style recipe generation script and one scope creep led to another and now I have this. Please download and extend the codebase (pls submit improvements on GitHub if you'd like).

If you want to skip all of that: I have built a **hosted version** you can access from your browser with a meticulously designed **web interface**. There is also **cURL-ur-MIRA** support and I handle all the infrastructure.

[miraos.org](https://miraos.org)

— Taylor Satula

---

**Thank you to:**

Boris from Claude Code (and his team), David Hahn (for unrelenting perseverance even to your own detriment), Sarah from MemGPT (for seeding the idea that we can let the robots manage their own context window), Myself (for taking the time to see it through and be willing to go back to the drawing board a dozen times until I was satisfied with the cohesive whole)
